{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rightmove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/NLHHbQyYsICRFEML7OPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrandonLow96/Property-valuation-visualisation/blob/main/Rightmove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDEAOYNihP6d"
      },
      "source": [
        "# Rightmove - Building a dataset of property listings\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, I will be demonstrating how to scrape property listings from Rightmove.co.uk, the UK's largest property listing website. This notebook will create a csv file with the following information for each property listing:\n",
        "\n",
        "\n",
        "*   Price\n",
        "*   Property link\n",
        "*   Number of bedrooms\n",
        "*   Address of property\n",
        "\n",
        "It is possible to dive deeper into each property and scrape specific information on each property, such as the description of the property, distance to nearest tube station and schools around the property. However, going into such detail for each property slows down the code quite a bit and I don't need such detailed information for my current project. I will leave it up to you to decide how much information you require. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pGSATpnhEgI"
      },
      "source": [
        "# importing our libraries\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Rightmove uses specific codes to describe each London borough, I have \n",
        "manually collected these codes by search for each borough individually.\n",
        "\"\"\"\n",
        "\n",
        "BOROUGHS = {\n",
        "    \"City of London\": \"5E61224\",\n",
        "    \"Barking and Dagenham\": \"5E61400\",\n",
        "    \"Barnet\": \"5E93929\",\n",
        "    \"Bexley\": \"5E93932\",\n",
        "    \"Brent\": \"5E93935\",\n",
        "    \"Bromley\": \"5E93938\",\n",
        "    \"Camden\": \"5E93941\",\n",
        "    \"Croydon\": \"5E93944\",\n",
        "    \"Ealing\": \"5E93947\",\n",
        "    \"Enfield\": \"5E93950\",\n",
        "    \"Greenwich\": \"5E61226\",\n",
        "    \"Hackney\": \"5E93953\",\n",
        "    \"Hammersmith and Fulham\": \"5E61407\",\n",
        "    \"Haringey\": \"5E61227\",\n",
        "    \"Harrow\": \"5E93956\",\n",
        "    \"Havering\": \"5E61228\",\n",
        "    \"Hillingdon\": \"5E93959\",\n",
        "    \"Hounslow\": \"5E93962\",\n",
        "    \"Islington\": \"5E93965\",\n",
        "    \"Kensington and Chelsea\": \"5E61229\",\n",
        "    \"Kingston upon Thames\": \"5E93968\",\n",
        "    \"Lambeth\": \"5E93971\",\n",
        "    \"Lewisham\": \"5E61413\",\n",
        "    \"Merton\": \"5E61414\",\n",
        "    \"Newham\": \"5E61231\",\n",
        "    \"Redbridge\": \"5E61537\",\n",
        "    \"Richmond upon Thames\": \"5E61415\",\n",
        "    \"Southwark\": \"5E61518\",\n",
        "    \"Sutton\": \"5E93974\",\n",
        "    \"Tower Hamlets\": \"5E61417\",\n",
        "    \"Waltham Forest\": \"5E61232\",\n",
        "    \"Wandsworth\": \"5E93977\",\n",
        "    \"Westminster\": \"5E93980\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqFqqDdDhgq5",
        "outputId": "fab4f976-a2b8-4635-a7d5-501c4c9038a3"
      },
      "source": [
        "\n",
        "def main():\n",
        "\n",
        "    # initialise index, this tracks the page number we are on. every additional page adds 24 to the index\n",
        "\n",
        "    # create lists to store our data\n",
        "    all_apartment_links = []\n",
        "    all_description = []\n",
        "    all_address = []\n",
        "    all_price = []\n",
        "\n",
        "    # apparently the maximum page limit for rightmove is 42\n",
        "    for borough in list(BOROUGHS.values()):\n",
        "\n",
        "        # initialise index, this tracks the page number we are on. every additional page adds 24 to the index\n",
        "        index = 0\n",
        "\n",
        "        key = [key for key, value in BOROUGHS.items() if value == borough]\n",
        "        print(f\"We are scraping the borough named: {key}\")\n",
        "        for pages in range(41):\n",
        "\n",
        "            # define our user headers\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\"\n",
        "            }\n",
        "\n",
        "            # the website changes if the you are on page 1 as compared to other pages\n",
        "            if index == 0:\n",
        "                rightmove = f\"https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%{borough}&sortType=6&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=\"\n",
        "\n",
        "            elif index != 0:\n",
        "                rightmove = f\"https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=REGION%{borough}&sortType=6&index={index}&propertyTypes=&includeSSTC=false&mustHave=&dontShow=&furnishTypes=&keywords=\"\n",
        "\n",
        "            # request our webpage\n",
        "            res = requests.get(rightmove, headers=headers)\n",
        "\n",
        "            # check status\n",
        "            res.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "            # This gets the list of apartments\n",
        "            apartments = soup.find_all(\"div\", class_=\"l-searchResult is-list\")\n",
        "\n",
        "            # This gets the number of listings\n",
        "            number_of_listings = soup.find(\n",
        "                \"span\", {\"class\": \"searchHeader-resultCount\"}\n",
        "            )\n",
        "            number_of_listings = number_of_listings.get_text()\n",
        "            number_of_listings = int(number_of_listings.replace(\",\", \"\"))\n",
        "\n",
        "            for i in range(len(apartments)):\n",
        "\n",
        "                # tracks which apartment we are on in the page\n",
        "                apartment_no = apartments[i]\n",
        "\n",
        "                # append link\n",
        "                apartment_info = apartment_no.find(\"a\", class_=\"propertyCard-link\")\n",
        "                link = \"https://www.rightmove.co.uk\" + apartment_info.attrs[\"href\"]\n",
        "                all_apartment_links.append(link)\n",
        "\n",
        "                # append address\n",
        "                address = (\n",
        "                    apartment_info.find(\"address\", class_=\"propertyCard-address\")\n",
        "                    .get_text()\n",
        "                    .strip()\n",
        "                )\n",
        "                all_address.append(address)\n",
        "\n",
        "                # append description\n",
        "                description = (\n",
        "                    apartment_info.find(\"h2\", class_=\"propertyCard-title\")\n",
        "                    .get_text()\n",
        "                    .strip()\n",
        "                )\n",
        "                all_description.append(description)\n",
        "\n",
        "                # append price\n",
        "                price = (\n",
        "                    apartment_no.find(\"div\", class_=\"propertyCard-priceValue\")\n",
        "                    .get_text()\n",
        "                    .strip()\n",
        "                )\n",
        "                all_price.append(price)\n",
        "\n",
        "            print(f\"You have scrapped {pages + 1} pages of apartment listings.\")\n",
        "            print(f\"You have {number_of_listings - index} listings left to go\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # code to ensure that we do not overwhelm the website\n",
        "            time.sleep(random.randint(1, 3))\n",
        "\n",
        "            # Code to count how many listings we have scrapped already.\n",
        "            index = index + 24\n",
        "\n",
        "            if index >= number_of_listings:\n",
        "                break\n",
        "\n",
        "    # convert data to dataframe\n",
        "    data = {\n",
        "        \"Links\": all_apartment_links,\n",
        "        \"Address\": all_address,\n",
        "        \"Description\": all_description,\n",
        "        \"Price\": all_price,\n",
        "    }\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "    df.to_csv(r\"sales_data.csv\", encoding=\"utf-8\", header=\"true\", index = False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1y9SMPhpgW",
        "outputId": "f66dd1ec-f810-4a10-a6aa-c80f28acf177"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbttxk1eh-uM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKyPiKqTh-7t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6vN9aMeh6y_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}